"""
ç½‘ç»œæ‹“æ‰‘ç‰¹æ€§åˆ†ææ¨¡å—
åˆ†æç”¨æˆ·-ç”µå½±äºŒéƒ¨å›¾çš„å¤æ‚ç½‘ç»œç‰¹æ€§
"""
import sys
from pathlib import Path
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib
# è®¾ç½®éäº¤äº’å¼åç«¯ï¼Œé€‚ç”¨äºæœåŠ¡å™¨ç¯å¢ƒï¼ˆæ— å›¾å½¢ç•Œé¢ï¼‰
matplotlib.use('Agg')  # å¿…é¡»åœ¨å¯¼å…¥ pyplot ä¹‹å‰è®¾ç½®
import matplotlib.pyplot as plt
from scipy import stats
from scipy.optimize import curve_fit
import warnings
from multiprocessing import Pool, cpu_count
from functools import partial
import time

# å°è¯•å¯¼å…¥GPUåŠ é€Ÿåº“ï¼ˆä½¿ç”¨PyTorchï¼Œæ›´å®¹æ˜“å®‰è£…ï¼‰
GPU_AVAILABLE = False
TORCH_AVAILABLE = False
try:
    import torch
    TORCH_AVAILABLE = True
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
    if torch.cuda.is_available():
        GPU_AVAILABLE = True
        print(f"âœ“ æ£€æµ‹åˆ°PyTorch GPUæ”¯æŒ (CUDA {torch.version.cuda})")
    else:
        print("âš  PyTorchå·²å®‰è£…ï¼Œä½†æœªæ£€æµ‹åˆ°GPUè®¾å¤‡")
except ImportError:
    TORCH_AVAILABLE = False
    GPU_AVAILABLE = False
    print("âš  PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUè®¡ç®—")
    print("  å¦‚éœ€GPUåŠ é€Ÿï¼Œè¯·å®‰è£…: pip install torch")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config import FILE_PATH

# ä½¿ç”¨æœ€åŸºç¡€çš„å­—ä½“è®¾ç½®ï¼Œç¡®ä¿æ–‡å­—èƒ½æ­£å¸¸æ˜¾ç¤º
# ç›´æ¥ä½¿ç”¨è‹±æ–‡æ ‡ç­¾ï¼Œé¿å…å­—ä½“é—®é¢˜
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans', 'sans-serif']
matplotlib.rcParams['axes.unicode_minus'] = False
CHINESE_FONT_AVAILABLE = False  # å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡æ ‡ç­¾

warnings.filterwarnings('ignore')


class NetworkAnalyzer:
    """ç½‘ç»œæ‹“æ‰‘ç‰¹æ€§åˆ†æå™¨"""
    
    def __init__(self, ratings_path=None, movies_path=None):
        """
        åˆå§‹åŒ–ç½‘ç»œåˆ†æå™¨
        
        Args:
            ratings_path: è¯„åˆ†æ•°æ®è·¯å¾„
            movies_path: ç”µå½±æ•°æ®è·¯å¾„
        """
        if ratings_path is None:
            ratings_path = FILE_PATH / "ratings.dat"
        if movies_path is None:
            movies_path = FILE_PATH / "movies.dat"
            
        self.ratings_path = ratings_path
        self.movies_path = movies_path
        self.G = None
        self.user_nodes = None
        self.movie_nodes = None
        self.stats = {}
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        # åŠ è½½è¯„åˆ†æ•°æ®
        self.ratings = pd.read_csv(
            self.ratings_path,
            sep='::',
            engine='python',
            names=['userId', 'movieId', 'rating', 'timestamp'],
            encoding='latin-1'
        )
        # åŠ è½½ç”µå½±æ•°æ®
        self.movies = pd.read_csv(
            self.movies_path,
            sep='::',
            engine='python',
            names=['movieId', 'title', 'genres'],
            encoding='latin-1'
        )
        print(f"åŠ è½½å®Œæˆ: {len(self.ratings)} æ¡è¯„åˆ†è®°å½•, {len(self.movies)} éƒ¨ç”µå½±")
        
    def build_bipartite_graph(self):
        """æ„å»ºç”¨æˆ·-ç”µå½±äºŒéƒ¨å›¾"""
        print("æ­£åœ¨æ„å»ºäºŒéƒ¨å›¾...")
        self.G = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
        for _, row in self.ratings.iterrows():
            user_node = f"U_{row['userId']}"
            movie_node = f"M_{row['movieId']}"
            
            # æ·»åŠ èŠ‚ç‚¹
            self.G.add_node(user_node, node_type='user', id=row['userId'])
            self.G.add_node(movie_node, node_type='movie', id=row['movieId'])
            
            # æ·»åŠ è¾¹ï¼ˆæƒé‡ä¸ºè¯„åˆ†ï¼‰
            self.G.add_edge(user_node, movie_node, weight=row['rating'])
        
        # åˆ†ç¦»ç”¨æˆ·èŠ‚ç‚¹å’Œç”µå½±èŠ‚ç‚¹
        self.user_nodes = [n for n in self.G.nodes() if n.startswith('U_')]
        self.movie_nodes = [n for n in self.G.nodes() if n.startswith('M_')]
        
        print(f"å›¾æ„å»ºå®Œæˆ: {self.G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {self.G.number_of_edges()} æ¡è¾¹")
        print(f"ç”¨æˆ·èŠ‚ç‚¹: {len(self.user_nodes)}, ç”µå½±èŠ‚ç‚¹: {len(self.movie_nodes)}")
        
    def calculate_basic_stats(self):
        """è®¡ç®—åŸºæœ¬ç»Ÿè®¡ç‰¹æ€§"""
        print("\n=== è®¡ç®—åŸºæœ¬ç»Ÿè®¡ç‰¹æ€§ ===")
        
        # èŠ‚ç‚¹æ•°å’Œè¾¹æ•°
        self.stats['num_nodes'] = self.G.number_of_nodes()
        self.stats['num_edges'] = self.G.number_of_edges()
        self.stats['num_users'] = len(self.user_nodes)
        self.stats['num_movies'] = len(self.movie_nodes)
        
        # å¹³å‡åº¦
        degrees = dict(self.G.degree())
        self.stats['avg_degree'] = np.mean(list(degrees.values()))
        self.stats['max_degree'] = max(degrees.values())
        self.stats['min_degree'] = min(degrees.values())
        
        # åº¦åˆ†å¸ƒ
        degree_sequence = sorted([d for n, d in self.G.degree()], reverse=True)
        self.stats['degree_sequence'] = degree_sequence
        
        print(f"èŠ‚ç‚¹æ•°: {self.stats['num_nodes']}")
        print(f"è¾¹æ•°: {self.stats['num_edges']}")
        print(f"å¹³å‡åº¦: {self.stats['avg_degree']:.2f}")
        print(f"æœ€å¤§åº¦: {self.stats['max_degree']}")
        print(f"æœ€å°åº¦: {self.stats['min_degree']}")
        
    def _compute_clustering_worker(self, args):
        """å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼šè®¡ç®—å•ä¸ªæŠ•å½±å›¾çš„èšç±»ç³»æ•°"""
        node_list, edges_data, node_type = args
        try:
            # é‡å»ºå­å›¾
            G_sub = nx.Graph()
            G_sub.add_nodes_from(node_list)
            for edge in edges_data:
                G_sub.add_edge(edge[0], edge[1])
            
            # æŠ•å½±å¹¶è®¡ç®—èšç±»ç³»æ•°
            if node_type == 'user':
                projection = nx.bipartite.projected_graph(G_sub, node_list)
            else:
                projection = nx.bipartite.projected_graph(G_sub, node_list)
            
            clustering = nx.average_clustering(projection)
            return (node_type, clustering, None)
        except Exception as e:
            return (node_type, None, str(e))
    
    def _compute_clustering_gpu_torch(self, nx_graph, node_list, node_type):
        """ä½¿ç”¨PyTorch GPUåŠ é€Ÿè®¡ç®—èšç±»ç³»æ•°"""
        if not GPU_AVAILABLE or not TORCH_AVAILABLE:
            return None
        
        try:
            print(f"  ğŸš€ ä½¿ç”¨PyTorch GPUåŠ é€Ÿè®¡ç®—{node_type}ç½‘ç»œèšç±»ç³»æ•°...")
            sys.stdout.flush()
            
            # å°†å›¾è½¬æ¢ä¸ºé‚»æ¥çŸ©é˜µï¼ˆä½¿ç”¨PyTorchï¼‰
            nodes = list(nx_graph.nodes())
            node_to_idx = {node: i for i, node in enumerate(nodes)}
            n = len(nodes)
            
            # åˆ›å»ºé‚»æ¥çŸ©é˜µï¼ˆåœ¨GPUä¸Šï¼‰
            device = torch.device('cuda')
            adj_matrix = torch.zeros((n, n), dtype=torch.float32, device=device)
            
            # å¡«å……é‚»æ¥çŸ©é˜µ
            for u, v in nx_graph.edges():
                i, j = node_to_idx[u], node_to_idx[v]
                adj_matrix[i, j] = 1.0
                adj_matrix[j, i] = 1.0  # æ— å‘å›¾
            
            # è®¡ç®—åº¦çŸ©é˜µ
            degrees = torch.sum(adj_matrix, dim=1)
            
            # è®¡ç®— A^3 çš„å¯¹è§’çº¿å…ƒç´ ï¼ˆä¸‰è§’å½¢æ•°é‡ï¼‰
            # å¯¹äºèŠ‚ç‚¹iï¼ŒA^3[i,i] = ä¸‰è§’å½¢æ•°é‡ * 2ï¼ˆæ¯ä¸ªä¸‰è§’å½¢è¢«è®¡ç®—ä¸¤æ¬¡ï¼‰
            adj_cubed = torch.mm(torch.mm(adj_matrix, adj_matrix), adj_matrix)
            triangles = torch.diag(adj_cubed) / 2.0  # é™¤ä»¥2å› ä¸ºæ¯ä¸ªä¸‰è§’å½¢è¢«è®¡ç®—ä¸¤æ¬¡
            
            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„èšç±»ç³»æ•°
            # C_i = 2 * triangles_i / (k_i * (k_i - 1))
            # é¿å…é™¤é›¶
            k = degrees.float()
            k_safe = torch.clamp(k * (k - 1), min=1.0)
            clustering_per_node = 2.0 * triangles / k_safe
            
            # åªå¯¹åº¦ >= 2 çš„èŠ‚ç‚¹è®¡ç®—å¹³å‡å€¼
            valid_mask = k >= 2
            if torch.sum(valid_mask) > 0:
                avg_clustering = torch.mean(clustering_per_node[valid_mask]).item()
                print(f"  âœ“ PyTorch GPUè®¡ç®—å®Œæˆ")
                sys.stdout.flush()
                return float(avg_clustering)
            else:
                return None
                
        except Exception as e:
            print(f"  PyTorch GPUè®¡ç®—å¤±è´¥: {e}ï¼Œå›é€€åˆ°CPU")
            sys.stdout.flush()
            return None
    
    def calculate_clustering_coefficient(self, use_sampling=True, sample_size=2000, num_processes=None, use_gpu=None):
        """è®¡ç®—èšç±»ç³»æ•°ï¼ˆæ”¯æŒGPUåŠ é€Ÿå’Œå¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—ï¼‰
        
        Args:
            use_sampling: æ˜¯å¦ä½¿ç”¨é‡‡æ ·æ–¹æ³•ï¼ˆå¯¹äºå¤§å‹ç½‘ç»œï¼‰
            sample_size: é‡‡æ ·èŠ‚ç‚¹æ•°é‡
            num_processes: è¿›ç¨‹æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨CPUæ ¸å¿ƒæ•°ï¼‰
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        print("\n=== è®¡ç®—èšç±»ç³»æ•° ===")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
        
        # æ£€æµ‹GPUå¯ç”¨æ€§
        if use_gpu is None:
            use_gpu = GPU_AVAILABLE
        
        if use_gpu and GPU_AVAILABLE:
            print("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—")
        else:
            if use_gpu and not GPU_AVAILABLE:
                print("âš  GPUä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUè®¡ç®—")
            else:
                print("ğŸ’» ä½¿ç”¨CPUè®¡ç®—")
        
        if num_processes is None:
            num_processes = max(1, cpu_count() - 1)  # ä¿ç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
        
        num_nodes = self.G.number_of_nodes()
        print(f"ç½‘ç»œèŠ‚ç‚¹æ•°: {num_nodes}")
        if not use_gpu:
            print(f"ä½¿ç”¨è¿›ç¨‹æ•°: {num_processes}")
        sys.stdout.flush()
        
        # å¯¹äºå¤§å‹ç½‘ç»œï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•ä»¥æé«˜é€Ÿåº¦
        if use_sampling and num_nodes > 5000:
            print(f"ç½‘ç»œè¾ƒå¤§ï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•ï¼ˆé‡‡æ · {sample_size} ä¸ªèŠ‚ç‚¹ï¼‰...")
            print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            sys.stdout.flush()
            
            try:
                # å‡†å¤‡ç”¨æˆ·ç½‘ç»œæ•°æ®
                print("\n[1/2] æ­£åœ¨å‡†å¤‡ç”¨æˆ·ç½‘ç»œæ•°æ®å¹¶è®¡ç®—èšç±»ç³»æ•°...")
                sys.stdout.flush()
                start_time = time.time()
                
                if len(self.user_nodes) > sample_size:
                    sampled_users = list(np.random.choice(self.user_nodes, sample_size, replace=False))
                    print(f"  é‡‡æ ·äº† {len(sampled_users)} ä¸ªç”¨æˆ·èŠ‚ç‚¹")
                else:
                    sampled_users = self.user_nodes
                    print(f"  ä½¿ç”¨å…¨éƒ¨ {len(sampled_users)} ä¸ªç”¨æˆ·èŠ‚ç‚¹")
                
                sys.stdout.flush()
                
                # æ„å»ºå­å›¾ï¼ˆåªåŒ…å«é‡‡æ ·èŠ‚ç‚¹å’Œæ‰€æœ‰ç”µå½±èŠ‚ç‚¹ï¼‰
                print("  æ­£åœ¨æ„å»ºç”¨æˆ·æŠ•å½±ç½‘ç»œ...")
                sys.stdout.flush()
                user_subgraph = self.G.subgraph(list(sampled_users) + self.movie_nodes)
                
                print("  æ­£åœ¨è®¡ç®—ç”¨æˆ·æŠ•å½±ç½‘ç»œèšç±»ç³»æ•°ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
                sys.stdout.flush()
                user_projection = nx.bipartite.projected_graph(user_subgraph, sampled_users)
                
                # å°è¯•ä½¿ç”¨GPUè®¡ç®—ï¼ˆPyTorchï¼‰
                if use_gpu and GPU_AVAILABLE and TORCH_AVAILABLE:
                    gpu_result = self._compute_clustering_gpu_torch(user_projection, sampled_users, 'User')
                    if gpu_result is not None:
                        global_clustering = gpu_result
                    else:
                        print("  ä½¿ç”¨CPUè®¡ç®—å¹³å‡èšç±»ç³»æ•°...")
                        sys.stdout.flush()
                        global_clustering = nx.average_clustering(user_projection)
                else:
                    print("  æ­£åœ¨è®¡ç®—å¹³å‡èšç±»ç³»æ•°...")
                    sys.stdout.flush()
                    global_clustering = nx.average_clustering(user_projection)
                
                elapsed = time.time() - start_time
                
                self.stats['global_clustering_user'] = global_clustering
                print(f"  âœ“ ç”¨æˆ·æŠ•å½±ç½‘ç»œå…¨å±€èšç±»ç³»æ•°: {global_clustering:.4f} (è€—æ—¶: {elapsed:.1f}ç§’)")
                sys.stdout.flush()
                
                # å‡†å¤‡ç”µå½±ç½‘ç»œæ•°æ®
                print("\n[2/2] æ­£åœ¨å‡†å¤‡ç”µå½±ç½‘ç»œæ•°æ®å¹¶è®¡ç®—èšç±»ç³»æ•°...")
                sys.stdout.flush()
                start_time = time.time()
                
                if len(self.movie_nodes) > sample_size:
                    sampled_movies = list(np.random.choice(self.movie_nodes, sample_size, replace=False))
                    print(f"  é‡‡æ ·äº† {len(sampled_movies)} ä¸ªç”µå½±èŠ‚ç‚¹")
                else:
                    sampled_movies = self.movie_nodes
                    print(f"  ä½¿ç”¨å…¨éƒ¨ {len(sampled_movies)} ä¸ªç”µå½±èŠ‚ç‚¹")
                
                sys.stdout.flush()
                
                # æ„å»ºå­å›¾
                print("  æ­£åœ¨æ„å»ºç”µå½±æŠ•å½±ç½‘ç»œ...")
                sys.stdout.flush()
                movie_subgraph = self.G.subgraph(list(sampled_movies) + self.user_nodes)
                
                print("  æ­£åœ¨è®¡ç®—ç”µå½±æŠ•å½±ç½‘ç»œèšç±»ç³»æ•°ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
                sys.stdout.flush()
                movie_projection = nx.bipartite.projected_graph(movie_subgraph, sampled_movies)
                
                # å°è¯•ä½¿ç”¨GPUè®¡ç®—ï¼ˆPyTorchï¼‰
                if use_gpu and GPU_AVAILABLE and TORCH_AVAILABLE:
                    gpu_result = self._compute_clustering_gpu_torch(movie_projection, sampled_movies, 'Movie')
                    if gpu_result is not None:
                        global_clustering_movie = gpu_result
                    else:
                        print("  ä½¿ç”¨CPUè®¡ç®—å¹³å‡èšç±»ç³»æ•°...")
                        sys.stdout.flush()
                        global_clustering_movie = nx.average_clustering(movie_projection)
                else:
                    print("  æ­£åœ¨è®¡ç®—å¹³å‡èšç±»ç³»æ•°...")
                    sys.stdout.flush()
                    global_clustering_movie = nx.average_clustering(movie_projection)
                
                elapsed = time.time() - start_time
                
                self.stats['global_clustering_movie'] = global_clustering_movie
                print(f"  âœ“ ç”µå½±æŠ•å½±ç½‘ç»œå…¨å±€èšç±»ç³»æ•°: {global_clustering_movie:.4f} (è€—æ—¶: {elapsed:.1f}ç§’)")
                sys.stdout.flush()
                    
            except Exception as e:
                print(f"è®¡ç®—èšç±»ç³»æ•°æ—¶å‡ºé”™: {e}")
                print("  å°è¯•ä½¿ç”¨æ›´å°çš„é‡‡æ ·...")
                sys.stdout.flush()
                try:
                    # ä½¿ç”¨æ›´å°çš„é‡‡æ ·ï¼ˆå¦‚æœåŸå§‹é‡‡æ ·å¤±è´¥ï¼Œå°è¯•æ›´å°çš„é‡‡æ ·ï¼‰
                    # ä½¿ç”¨åŸå§‹ sample_size çš„ä¸€åŠï¼Œä½†ä¸è¶…è¿‡ 2000ï¼Œè‡³å°‘ 100
                    small_sample = min(2000, max(100, sample_size // 2))
                    if len(self.user_nodes) > small_sample:
                        sampled_users = list(np.random.choice(self.user_nodes, small_sample, replace=False))
                        user_subgraph = self.G.subgraph(list(sampled_users) + self.movie_nodes)
                        user_projection = nx.bipartite.projected_graph(user_subgraph, sampled_users)
                        global_clustering = nx.average_clustering(user_projection)
                        self.stats['global_clustering_user'] = global_clustering
                        print(f"  âœ“ ç”¨æˆ·ç½‘ç»œèšç±»ç³»æ•°: {global_clustering:.4f} (å°é‡‡æ ·)")
                    else:
                        self.stats['global_clustering_user'] = 0
                    
                    if len(self.movie_nodes) > small_sample:
                        sampled_movies = list(np.random.choice(self.movie_nodes, small_sample, replace=False))
                        movie_subgraph = self.G.subgraph(list(sampled_movies) + self.user_nodes)
                        movie_projection = nx.bipartite.projected_graph(movie_subgraph, sampled_movies)
                        global_clustering_movie = nx.average_clustering(movie_projection)
                        self.stats['global_clustering_movie'] = global_clustering_movie
                        print(f"  âœ“ ç”µå½±ç½‘ç»œèšç±»ç³»æ•°: {global_clustering_movie:.4f} (å°é‡‡æ ·)")
                    else:
                        self.stats['global_clustering_movie'] = 0
                except Exception as e2:
                    print(f"  ä½¿ç”¨å°é‡‡æ ·ä¹Ÿå¤±è´¥: {e2}")
                    self.stats['global_clustering_user'] = 0
                    self.stats['global_clustering_movie'] = 0
                sys.stdout.flush()
        else:
            # å°ç½‘ç»œæˆ–ç¦ç”¨é‡‡æ ·æ—¶ï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨èŠ‚ç‚¹è®¡ç®—
            if not use_sampling:
                print("  ä½¿ç”¨å…¨éƒ¨èŠ‚ç‚¹è®¡ç®—ï¼ˆç¦ç”¨é‡‡æ ·æ¨¡å¼ï¼‰...")
            else:
                print(f"  ç½‘ç»œèŠ‚ç‚¹æ•° ({num_nodes}) <= 5000ï¼Œä½¿ç”¨å…¨éƒ¨èŠ‚ç‚¹è®¡ç®—...")
            sys.stdout.flush()
            
            try:
                print("  æ­£åœ¨è®¡ç®—ç”¨æˆ·æŠ•å½±ç½‘ç»œèšç±»ç³»æ•°...")
                sys.stdout.flush()
                start_time = time.time()
                user_projection = nx.bipartite.projected_graph(self.G, self.user_nodes)
                
                # å°è¯•ä½¿ç”¨GPUè®¡ç®—
                if use_gpu and GPU_AVAILABLE and TORCH_AVAILABLE:
                    gpu_result = self._compute_clustering_gpu_torch(user_projection, self.user_nodes, 'User')
                    if gpu_result is not None:
                        global_clustering = gpu_result
                    else:
                        global_clustering = nx.average_clustering(user_projection)
                else:
                    global_clustering = nx.average_clustering(user_projection)
                
                elapsed = time.time() - start_time
                self.stats['global_clustering_user'] = global_clustering
                print(f"  âœ“ ç”¨æˆ·æŠ•å½±ç½‘ç»œå…¨å±€èšç±»ç³»æ•°: {global_clustering:.4f} (è€—æ—¶: {elapsed:.1f}ç§’)")
                sys.stdout.flush()
                
                print("  æ­£åœ¨è®¡ç®—ç”µå½±æŠ•å½±ç½‘ç»œèšç±»ç³»æ•°...")
                sys.stdout.flush()
                start_time = time.time()
                movie_projection = nx.bipartite.projected_graph(self.G, self.movie_nodes)
                
                # å°è¯•ä½¿ç”¨GPUè®¡ç®—
                if use_gpu and GPU_AVAILABLE and TORCH_AVAILABLE:
                    gpu_result = self._compute_clustering_gpu_torch(movie_projection, self.movie_nodes, 'Movie')
                    if gpu_result is not None:
                        global_clustering_movie = gpu_result
                    else:
                        global_clustering_movie = nx.average_clustering(movie_projection)
                else:
                    global_clustering_movie = nx.average_clustering(movie_projection)
                
                elapsed = time.time() - start_time
                self.stats['global_clustering_movie'] = global_clustering_movie
                print(f"  âœ“ ç”µå½±æŠ•å½±ç½‘ç»œå…¨å±€èšç±»ç³»æ•°: {global_clustering_movie:.4f} (è€—æ—¶: {elapsed:.1f}ç§’)")
                sys.stdout.flush()
            except Exception as e:
                print(f"è®¡ç®—èšç±»ç³»æ•°æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                self.stats['global_clustering_user'] = None
                self.stats['global_clustering_movie'] = None
                sys.stdout.flush()
        
        print("èšç±»ç³»æ•°è®¡ç®—å®Œæˆï¼\n")
        sys.stdout.flush()
    
    def calculate_path_length(self):
        """è®¡ç®—è·¯å¾„é•¿åº¦"""
        print("\n=== è®¡ç®—è·¯å¾„é•¿åº¦ ===")
        
        # å¯¹äºå¤§å‹ç½‘ç»œï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•
        if self.G.number_of_nodes() > 5000:
            print("ç½‘ç»œè¾ƒå¤§ï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•è®¡ç®—è·¯å¾„é•¿åº¦...")
            sample_size = min(2000, len(self.user_nodes))
            sampled_users = np.random.choice(self.user_nodes, sample_size, replace=False)
            
            path_lengths = []
            for i, u1 in enumerate(sampled_users):
                if i % 100 == 0:
                    print(f"  å¤„ç†è¿›åº¦: {i}/{sample_size}")
                for u2 in sampled_users[i+1:]:
                    try:
                        path = nx.shortest_path_length(self.G, u1, u2)
                        path_lengths.append(path)
                    except nx.NetworkXNoPath:
                        continue
            
            if path_lengths:
                self.stats['avg_path_length'] = np.mean(path_lengths)
                self.stats['diameter'] = max(path_lengths)
                print(f"å¹³å‡è·¯å¾„é•¿åº¦: {self.stats['avg_path_length']:.2f}")
                print(f"ç½‘ç»œç›´å¾„: {self.stats['diameter']}")
            else:
                print("æ— æ³•è®¡ç®—è·¯å¾„é•¿åº¦ï¼ˆç½‘ç»œå¯èƒ½ä¸è¿é€šï¼‰")
                self.stats['avg_path_length'] = None
                self.stats['diameter'] = None
        else:
            # å°ç½‘ç»œç›´æ¥è®¡ç®—
            try:
                self.stats['avg_path_length'] = nx.average_shortest_path_length(self.G)
                self.stats['diameter'] = nx.diameter(self.G)
                print(f"å¹³å‡è·¯å¾„é•¿åº¦: {self.stats['avg_path_length']:.2f}")
                print(f"ç½‘ç»œç›´å¾„: {self.stats['diameter']}")
            except nx.NetworkXError as e:
                print(f"ç½‘ç»œä¸è¿é€š: {e}")
                # è®¡ç®—æœ€å¤§è¿é€šå­å›¾çš„è·¯å¾„é•¿åº¦
                largest_cc = max(nx.connected_components(self.G), key=len)
                subgraph = self.G.subgraph(largest_cc)
                self.stats['avg_path_length'] = nx.average_shortest_path_length(subgraph)
                self.stats['diameter'] = nx.diameter(subgraph)
                print(f"æœ€å¤§è¿é€šå­å›¾å¹³å‡è·¯å¾„é•¿åº¦: {self.stats['avg_path_length']:.2f}")
                print(f"æœ€å¤§è¿é€šå­å›¾ç›´å¾„: {self.stats['diameter']}")
    
    def analyze_degree_distribution(self):
        """åˆ†æåº¦åˆ†å¸ƒï¼ˆæ£€æµ‹å¹‚å¾‹åˆ†å¸ƒï¼‰"""
        print("\n=== åˆ†æåº¦åˆ†å¸ƒ ===")
        
        degrees = dict(self.G.degree())
        degree_values = list(degrees.values())
        
        # åº¦åˆ†å¸ƒç»Ÿè®¡
        unique_degrees, counts = np.unique(degree_values, return_counts=True)
        degree_dist = dict(zip(unique_degrees, counts))
        
        self.stats['degree_distribution'] = degree_dist
        
        # å¹‚å¾‹åˆ†å¸ƒæ£€æµ‹
        # ä½¿ç”¨ä¸å¯è§†åŒ–ç›¸åŒçš„ç›´æ¥åº¦åˆ†å¸ƒæ–¹æ³•ï¼Œç¡®ä¿ç»“æœä¸€è‡´
        # åªè€ƒè™‘åº¦ >= 1 çš„èŠ‚ç‚¹
        unique_degrees_filtered = unique_degrees[unique_degrees >= 1]
        counts_filtered = counts[unique_degrees >= 1]
        
        if len(unique_degrees_filtered) > 10:
            try:
                # åœ¨å¯¹æ•°ç©ºé—´è¿›è¡Œçº¿æ€§æ‹Ÿåˆ
                # å¯¹äºå¹‚å¾‹åˆ†å¸ƒï¼šcount = C * degree^(-gamma)
                # æ‰€ä»¥ log(count) = log(C) - gamma * log(degree)
                log_degrees = np.log(unique_degrees_filtered)
                log_counts = np.log(counts_filtered)
                
                # å»é™¤æ— ç©·å¤§å’ŒNaN
                valid_mask = np.isfinite(log_degrees) & np.isfinite(log_counts) & (log_counts > -np.inf)
                if np.sum(valid_mask) > 5:
                    log_degrees_valid = log_degrees[valid_mask]
                    log_counts_valid = log_counts[valid_mask]
                    
                    # çº¿æ€§æ‹Ÿåˆï¼šlog(count) = a * log(degree) + b
                    # æ–œç‡åº”è¯¥æ˜¯è´Ÿæ•°ï¼Œgamma = -slope
                    slope, intercept = np.polyfit(log_degrees_valid, log_counts_valid, 1)
                    power_law_exponent = -slope  # å¹‚å¾‹æŒ‡æ•°ï¼ˆåº”è¯¥æ˜¯æ­£æ•°ï¼‰
                    
                    self.stats['power_law_exponent'] = power_law_exponent
                    self.stats['is_power_law'] = power_law_exponent > 1.0  # å¹‚å¾‹æŒ‡æ•°é€šå¸¸ > 1.0
                    
                    print(f"å¹‚å¾‹æŒ‡æ•° (gamma): {power_law_exponent:.4f}")
                    print(f"æ˜¯å¦ä¸ºå¹‚å¾‹åˆ†å¸ƒ: {self.stats['is_power_law']}")
                else:
                    self.stats['power_law_exponent'] = None
                    self.stats['is_power_law'] = False
            except Exception as e:
                print(f"å¹‚å¾‹æ‹Ÿåˆå¤±è´¥: {e}")
                self.stats['power_law_exponent'] = None
                self.stats['is_power_law'] = False
        else:
            self.stats['power_law_exponent'] = None
            self.stats['is_power_law'] = False
    
    def calculate_degree_correlation(self):
        """è®¡ç®—åº¦ç›¸å…³æ€§ï¼ˆåŒé…æ€§/å¼‚é…æ€§ï¼‰"""
        print("\n=== è®¡ç®—åº¦ç›¸å…³æ€§ ===")
        
        try:
            # è®¡ç®—åº¦ç›¸å…³æ€§ï¼ˆPearsonç›¸å…³ç³»æ•°ï¼‰
            degrees = dict(self.G.degree())
            edges = list(self.G.edges())
            
            if len(edges) > 0:
                edge_degrees = [(degrees[u], degrees[v]) for u, v in edges]
                degrees_u = [d[0] for d in edge_degrees]
                degrees_v = [d[1] for d in edge_degrees]
                
                correlation = np.corrcoef(degrees_u, degrees_v)[0, 1]
                self.stats['degree_correlation'] = correlation
                
                if correlation > 0:
                    assortativity_type = "åŒé…æ€§ (Assortative)"
                elif correlation < 0:
                    assortativity_type = "å¼‚é…æ€§ (Disassortative)"
                else:
                    assortativity_type = "ä¸­æ€§ (Neutral)"
                
                print(f"åº¦ç›¸å…³æ€§: {correlation:.4f} ({assortativity_type})")
            else:
                self.stats['degree_correlation'] = None
        except Exception as e:
            print(f"è®¡ç®—åº¦ç›¸å…³æ€§æ—¶å‡ºé”™: {e}")
            self.stats['degree_correlation'] = None
    
    def identify_key_nodes(self, top_k=10):
        """è¯†åˆ«å…³é”®èŠ‚ç‚¹ï¼ˆé«˜å½±å“åŠ›ç”¨æˆ·/ç”µå½±ï¼‰"""
        print(f"\n=== è¯†åˆ«Top-{top_k}å…³é”®èŠ‚ç‚¹ ===")
        
        degrees = dict(self.G.degree())
        
        # æŒ‰åº¦æ’åº
        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        
        # ç”¨æˆ·èŠ‚ç‚¹
        user_key_nodes = [(n, d) for n, d in sorted_nodes if n.startswith('U_')][:top_k]
        # ç”µå½±èŠ‚ç‚¹
        movie_key_nodes = [(n, d) for n, d in sorted_nodes if n.startswith('M_')][:top_k]
        
        self.stats['top_users'] = user_key_nodes
        self.stats['top_movies'] = movie_key_nodes
        
        print(f"\nTop-{top_k} é«˜å½±å“åŠ›ç”¨æˆ·:")
        for i, (node, degree) in enumerate(user_key_nodes, 1):
            user_id = node.replace('U_', '')
            print(f"  {i}. ç”¨æˆ· {user_id}: åº¦ = {degree}")
        
        print(f"\nTop-{top_k} é«˜å½±å“åŠ›ç”µå½±:")
        for i, (node, degree) in enumerate(movie_key_nodes, 1):
            movie_id = node.replace('M_', '')
            # æŸ¥æ‰¾ç”µå½±åç§°
            movie_info = self.movies[self.movies['movieId'] == int(movie_id)]
            if not movie_info.empty:
                title = movie_info.iloc[0]['title']
                print(f"  {i}. {title} (ID: {movie_id}): åº¦ = {degree}")
            else:
                print(f"  {i}. ç”µå½± {movie_id}: åº¦ = {degree}")
    
    def visualize_degree_distribution(self, save_path='network_degree_distribution.png'):
        """å¯è§†åŒ–åº¦åˆ†å¸ƒ"""
        print(f"\n=== å¯è§†åŒ–åº¦åˆ†å¸ƒ ===")
        
        # ç›´æ¥ä½¿ç”¨è‹±æ–‡æ ‡ç­¾ï¼Œç¡®ä¿åœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½èƒ½æ­£å¸¸æ˜¾ç¤º
        labels = {
            'degree': 'Degree',
            'count': 'Node Count',
            'hist_title': 'Degree Distribution Histogram',
            'loglog_title': 'Degree Distribution (Log-Log Plot)',
            'loglog_x': 'Degree (Log Scale)',
            'loglog_y': 'Node Count (Log Scale)',
            'compare_title': 'User vs Movie Degree Distribution',
            'user': 'User',
            'movie': 'Movie',
            'ccdf_title': 'Complementary Cumulative Distribution (CCDF)',
            'ccdf_y': 'P(X >= k) (Log Scale)',
            'fit_label': 'Power Law Fit'
        }
        
        degrees = dict(self.G.degree())
        degree_values = list(degrees.values())
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. åº¦åˆ†å¸ƒç›´æ–¹å›¾
        ax1 = axes[0, 0]
        ax1.hist(degree_values, bins=50, edgecolor='black', alpha=0.7)
        ax1.set_xlabel(labels['degree'], fontsize=12)
        ax1.set_ylabel(labels['count'], fontsize=12)
        ax1.set_title(labels['hist_title'], fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # 2. å¯¹æ•°-å¯¹æ•°åº¦åˆ†å¸ƒï¼ˆæ£€æµ‹å¹‚å¾‹ï¼‰
        ax2 = axes[0, 1]
        unique_degrees, counts = np.unique(degree_values, return_counts=True)
        # è¿‡æ»¤æ‰0åº¦èŠ‚ç‚¹
        valid_idx = unique_degrees > 0
        unique_degrees = unique_degrees[valid_idx]
        counts = counts[valid_idx]
        
        ax2.loglog(unique_degrees, counts, 'bo', markersize=4, alpha=0.6, label='Data')
        ax2.set_xlabel(labels['loglog_x'], fontsize=12)
        ax2.set_ylabel(labels['loglog_y'], fontsize=12)
        ax2.set_title(labels['loglog_title'], fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3, which='both')
        
        # å¦‚æœæœ‰å¹‚å¾‹æŒ‡æ•°ï¼Œç»˜åˆ¶æ‹Ÿåˆçº¿
        # ä½¿ç”¨å­˜å‚¨çš„gammaå€¼ï¼Œç¡®ä¿ä¸æŠ¥å‘Šä¸­ä¸€è‡´
        if self.stats.get('power_law_exponent') is not None:
            gamma = self.stats['power_law_exponent']
            print(f"  ç»˜åˆ¶å¹‚å¾‹æ‹Ÿåˆçº¿ï¼ŒæŒ‡æ•° Î³ = {gamma:.4f} (ä½¿ç”¨å­˜å‚¨çš„å€¼)")
            
            # ä½¿ç”¨å­˜å‚¨çš„gammaå€¼è®¡ç®—æ‹Ÿåˆçº¿
            # å¯¹äºå¹‚å¾‹åˆ†å¸ƒï¼šcount = C * degree^(-gamma)
            # æ‰€ä»¥ log(count) = log(C) - gamma * log(degree)
            # æˆ‘ä»¬éœ€è¦æ ¹æ®æ•°æ®ç‚¹è®¡ç®—æˆªè· log(C)
            log_degrees = np.log(unique_degrees[unique_degrees > 0])
            log_counts = np.log(counts[unique_degrees > 0])
            
            # å»é™¤æ— ç©·å¤§å’ŒNaN
            valid_mask = np.isfinite(log_degrees) & np.isfinite(log_counts) & (log_counts > -np.inf)
            if np.sum(valid_mask) > 5:
                log_degrees_valid = log_degrees[valid_mask]
                log_counts_valid = log_counts[valid_mask]
                
                # ä½¿ç”¨å­˜å‚¨çš„gammaå€¼ï¼Œè®¡ç®—æˆªè·
                # log(count) = log(C) - gamma * log(degree)
                # æ‰€ä»¥ log(C) = log(count) + gamma * log(degree)
                # ä½¿ç”¨æœ€å°äºŒä¹˜æ³•è®¡ç®—æœ€ä½³æˆªè·
                log_C_values = log_counts_valid + gamma * log_degrees_valid
                log_C = np.mean(log_C_values)  # ä½¿ç”¨å¹³å‡å€¼ä½œä¸ºæˆªè·
                intercept = log_C
                
                # ç”Ÿæˆæ‹Ÿåˆçº¿
                # ä½¿ç”¨æ•°æ®ç‚¹çš„å®é™…èŒƒå›´
                x_min = unique_degrees[valid_mask].min()
                x_max = unique_degrees[valid_mask].max()
                x_fit = np.logspace(np.log10(x_min), np.log10(x_max), 200)
                
                # è®¡ç®—æ‹Ÿåˆçº¿ï¼šy = exp(intercept) * x^(-gamma)
                y_fit = np.exp(intercept) * (x_fit ** (-gamma))
                
                # åªç§»é™¤æ˜æ˜¾å¼‚å¸¸çš„å€¼ï¼ˆè´Ÿå€¼æˆ–æ— ç©·å¤§ï¼‰
                valid_fit_mask = (y_fit > 0) & np.isfinite(y_fit) & (y_fit <= counts.max() * 50)
                
                if np.sum(valid_fit_mask) > 10:
                    # ç»˜åˆ¶æ‹Ÿåˆçº¿ï¼ˆåªç»˜åˆ¶æœ‰æ•ˆéƒ¨åˆ†ï¼‰
                    ax2.plot(x_fit[valid_fit_mask], y_fit[valid_fit_mask], 'r--', linewidth=2.5, 
                            label=f"{labels['fit_label']} (Î³={gamma:.2f})")
                    ax2.legend(fontsize=10)
                else:
                    # å¦‚æœè¿‡æ»¤åç‚¹å¤ªå°‘ï¼Œç›´æ¥ç»˜åˆ¶å…¨éƒ¨ï¼ˆä¸è£å‰ªï¼‰
                    ax2.plot(x_fit, y_fit, 'r--', linewidth=2.5, 
                            label=f"{labels['fit_label']} (Î³={gamma:.2f})")
                    ax2.legend(fontsize=10)
            else:
                # å¦‚æœæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨ç®€åŒ–çš„æ‹Ÿåˆçº¿
                print("  ä½¿ç”¨ç®€åŒ–çš„æ‹Ÿåˆçº¿")
                x_fit = np.logspace(np.log10(unique_degrees.min()), 
                                  np.log10(unique_degrees.max()), 200)
                # ä½¿ç”¨ç»Ÿè®¡çš„gammaå€¼
                # count = C * degree^(-gamma)ï¼Œæ‰€ä»¥æ‹Ÿåˆçº¿åº”è¯¥å‘ä¸‹å€¾æ–œ
                # ä»æ•°æ®ä¸­ä¼°è®¡å¸¸æ•°Cï¼šåœ¨æœ€å°åº¦å€¼å¤„ï¼Œcountåº”è¯¥æ¥è¿‘counts.max()
                C = counts.max() * (unique_degrees.min() ** gamma)
                y_fit = C * (x_fit ** (-gamma))
                # åªç§»é™¤å¼‚å¸¸å€¼ï¼Œä¸è¦è¿‡åº¦è£å‰ª
                valid_fit = (y_fit > 0) & np.isfinite(y_fit)
                ax2.plot(x_fit[valid_fit], y_fit[valid_fit], 'r--', linewidth=2.5, 
                        label=f"{labels['fit_label']} (Î³={gamma:.2f})")
                ax2.legend(fontsize=10)
        else:
            print("  æœªè®¡ç®—å¹‚å¾‹æŒ‡æ•°ï¼Œè·³è¿‡æ‹Ÿåˆçº¿ç»˜åˆ¶")
        
        # 3. ç”¨æˆ·å’Œç”µå½±åº¦åˆ†å¸ƒå¯¹æ¯”
        ax3 = axes[1, 0]
        user_degrees = [degrees[n] for n in self.user_nodes]
        movie_degrees = [degrees[n] for n in self.movie_nodes]
        
        ax3.hist(user_degrees, bins=30, alpha=0.6, label=labels['user'], color='blue', edgecolor='black')
        ax3.hist(movie_degrees, bins=30, alpha=0.6, label=labels['movie'], color='red', edgecolor='black')
        ax3.set_xlabel(labels['degree'], fontsize=12)
        ax3.set_ylabel(labels['count'], fontsize=12)
        ax3.set_title(labels['compare_title'], fontsize=14, fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ç´¯ç§¯åº¦åˆ†å¸ƒï¼ˆCCDFï¼‰
        ax4 = axes[1, 1]
        # æ­£ç¡®è®¡ç®—CCDFï¼šå¯¹äºæ¯ä¸ªå”¯ä¸€çš„åº¦å€¼kï¼Œè®¡ç®—åº¦å€¼>=kçš„èŠ‚ç‚¹æ¯”ä¾‹
        sorted_degrees = np.sort(degree_values)
        unique_degrees_sorted = np.unique(sorted_degrees)
        
        # å¯¹äºæ¯ä¸ªå”¯ä¸€çš„åº¦å€¼ï¼Œè®¡ç®—æœ‰å¤šå°‘èŠ‚ç‚¹çš„åº¦å€¼ >= è¯¥åº¦å€¼
        ccdf_values = []
        for k in unique_degrees_sorted:
            count_ge_k = np.sum(sorted_degrees >= k)
            ccdf_k = count_ge_k / len(sorted_degrees)
            ccdf_values.append(ccdf_k)
        
        ccdf_values = np.array(ccdf_values)
        
        # è¿‡æ»¤æ‰CCDFä¸º0çš„å€¼ï¼ˆåœ¨å¯¹æ•°ç©ºé—´ä¸­æ— æ³•æ˜¾ç¤ºï¼‰
        valid_ccdf_mask = ccdf_values > 0
        unique_degrees_ccdf = unique_degrees_sorted[valid_ccdf_mask]
        ccdf_values = ccdf_values[valid_ccdf_mask]
        
        ax4.loglog(unique_degrees_ccdf, ccdf_values, 'b-', linewidth=2, alpha=0.7, label='CCDF')
        
        # å¦‚æœæœ‰å¹‚å¾‹æŒ‡æ•°ï¼Œç»˜åˆ¶ç†è®ºCCDFæ‹Ÿåˆçº¿
        # å¦‚æœ P(k) ~ k^(-Î³)ï¼Œåˆ™ CCDF(k) ~ k^(-Î³+1)
        if self.stats.get('power_law_exponent') is not None:
            gamma = self.stats['power_law_exponent']
            ccdf_exponent = -(gamma - 1)  # CCDFçš„å¹‚å¾‹æŒ‡æ•° = -(Î³-1)
            
            print(f"  ç»˜åˆ¶CCDFç†è®ºæ‹Ÿåˆçº¿ï¼ŒCCDFæŒ‡æ•° = {ccdf_exponent:.4f} (åŸºäº Î³={gamma:.4f})")
            
            # è®¡ç®—CCDFæ‹Ÿåˆçº¿çš„æˆªè·
            if len(unique_degrees_ccdf) > 5:
                log_degrees_ccdf = np.log(unique_degrees_ccdf)
                log_ccdf = np.log(ccdf_values)
                
                valid_ccdf_fit_mask = np.isfinite(log_degrees_ccdf) & np.isfinite(log_ccdf) & (log_ccdf > -np.inf)
                if np.sum(valid_ccdf_fit_mask) > 5:
                    log_degrees_ccdf_valid = log_degrees_ccdf[valid_ccdf_fit_mask]
                    log_ccdf_valid = log_ccdf[valid_ccdf_fit_mask]
                    
                    # ä½¿ç”¨ç†è®ºæŒ‡æ•°è®¡ç®—æˆªè·ï¼šlog(CCDF) = log(C) + ccdf_exponent * log(degree)
                    # æ‰€ä»¥ log(C) = log(CCDF) - ccdf_exponent * log(degree)
                    # ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œç»™ä¸­é—´åŒºåŸŸï¼ˆæ›´ç¬¦åˆå¹‚å¾‹çš„åŒºåŸŸï¼‰æ›´é«˜æƒé‡
                    # ä¸­é—´åŒºåŸŸé€šå¸¸æ˜¯åº¦å€¼çš„ä¸­ä½æ•°é™„è¿‘ï¼Œè¿™äº›åŒºåŸŸæ›´ç¬¦åˆå¹‚å¾‹å‡è®¾
                    median_idx = len(log_degrees_ccdf_valid) // 2
                    # ä½¿ç”¨é«˜æ–¯æƒé‡ï¼Œä¸­å¿ƒåŒºåŸŸæƒé‡æ›´é«˜
                    indices = np.arange(len(log_degrees_ccdf_valid))
                    weights = np.exp(-0.5 * ((indices - median_idx) / (len(log_degrees_ccdf_valid) / 4)) ** 2)
                    weights = weights / weights.sum()  # å½’ä¸€åŒ–æƒé‡
                    
                    log_C_ccdf_values = log_ccdf_valid - ccdf_exponent * log_degrees_ccdf_valid
                    log_C_ccdf = np.average(log_C_ccdf_values, weights=weights)  # ä½¿ç”¨åŠ æƒå¹³å‡ä½œä¸ºæˆªè·
                    
                    # ç”Ÿæˆæ‹Ÿåˆçº¿
                    x_min_ccdf = unique_degrees_ccdf[valid_ccdf_fit_mask].min()
                    x_max_ccdf = unique_degrees_ccdf[valid_ccdf_fit_mask].max()
                    x_fit_ccdf = np.logspace(np.log10(x_min_ccdf), np.log10(x_max_ccdf), 200)
                    
                    # CCDFæ‹Ÿåˆçº¿ï¼šy = exp(log_C_ccdf) * x^(ccdf_exponent)
                    y_fit_ccdf = np.exp(log_C_ccdf) * (x_fit_ccdf ** ccdf_exponent)
                    
                    # è¿‡æ»¤æœ‰æ•ˆå€¼
                    valid_fit_ccdf_mask = (y_fit_ccdf > 0) & np.isfinite(y_fit_ccdf) & (y_fit_ccdf <= 1.0)
                    
                    if np.sum(valid_fit_ccdf_mask) > 10:
                        ax4.plot(x_fit_ccdf[valid_fit_ccdf_mask], y_fit_ccdf[valid_fit_ccdf_mask], 
                                'r--', linewidth=2.5, alpha=0.8,
                                label=f'CCDF Fit (exp={ccdf_exponent:.2f})')
                        ax4.legend(fontsize=10)
        
        ax4.set_xlabel(labels['loglog_x'], fontsize=12)
        ax4.set_ylabel(labels['ccdf_y'], fontsize=12)
        ax4.set_title(labels['ccdf_title'], fontsize=14, fontweight='bold')
        ax4.grid(True, alpha=0.3, which='both')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path}")
        plt.close()
    
    def generate_report(self, save_path='network_analysis_report.txt'):
        """ç”Ÿæˆç½‘ç»œåˆ†ææŠ¥å‘Š"""
        print(f"\n=== ç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
        
        report = []
        report.append("=" * 60)
        report.append("ç½‘ç»œæ‹“æ‰‘ç‰¹æ€§åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")
        
        # åŸºæœ¬ç»Ÿè®¡
        report.append("ã€åŸºæœ¬ç»Ÿè®¡ç‰¹æ€§ã€‘")
        report.append(f"èŠ‚ç‚¹æ€»æ•°: {self.stats['num_nodes']}")
        report.append(f"è¾¹æ€»æ•°: {self.stats['num_edges']}")
        report.append(f"ç”¨æˆ·èŠ‚ç‚¹æ•°: {self.stats['num_users']}")
        report.append(f"ç”µå½±èŠ‚ç‚¹æ•°: {self.stats['num_movies']}")
        report.append(f"å¹³å‡åº¦: {self.stats['avg_degree']:.4f}")
        report.append(f"æœ€å¤§åº¦: {self.stats['max_degree']}")
        report.append(f"æœ€å°åº¦: {self.stats['min_degree']}")
        report.append("")
        
        # èšç±»ç³»æ•°
        report.append("ã€èšç±»ç³»æ•°ã€‘")
        if 'global_clustering_user' in self.stats:
            report.append(f"ç”¨æˆ·æŠ•å½±ç½‘ç»œå…¨å±€èšç±»ç³»æ•°: {self.stats['global_clustering_user']:.4f}")
        if 'global_clustering_movie' in self.stats:
            report.append(f"ç”µå½±æŠ•å½±ç½‘ç»œå…¨å±€èšç±»ç³»æ•°: {self.stats['global_clustering_movie']:.4f}")
        report.append("")
        
        # è·¯å¾„é•¿åº¦
        report.append("ã€è·¯å¾„é•¿åº¦ã€‘")
        if self.stats.get('avg_path_length') is not None:
            report.append(f"å¹³å‡è·¯å¾„é•¿åº¦: {self.stats['avg_path_length']:.4f}")
        if self.stats.get('diameter') is not None:
            report.append(f"ç½‘ç»œç›´å¾„: {self.stats['diameter']}")
        report.append("")
        
        # åº¦åˆ†å¸ƒ
        report.append("ã€åº¦åˆ†å¸ƒç‰¹æ€§ã€‘")
        if self.stats.get('power_law_exponent') is not None:
            report.append(f"å¹‚å¾‹æŒ‡æ•° (Î³): {self.stats['power_law_exponent']:.4f}")
            report.append(f"æ˜¯å¦ä¸ºå¹‚å¾‹åˆ†å¸ƒ: {self.stats['is_power_law']}")
        else:
            report.append("æ— æ³•ç¡®å®šæ˜¯å¦ä¸ºå¹‚å¾‹åˆ†å¸ƒ")
        report.append("")
        
        # åº¦ç›¸å…³æ€§
        report.append("ã€åº¦ç›¸å…³æ€§ã€‘")
        if self.stats.get('degree_correlation') is not None:
            corr = self.stats['degree_correlation']
            if corr > 0:
                report.append(f"åº¦ç›¸å…³æ€§: {corr:.4f} (åŒé…æ€§ - é«˜åº¦èŠ‚ç‚¹å€¾å‘äºè¿æ¥é«˜åº¦èŠ‚ç‚¹)")
            elif corr < 0:
                report.append(f"åº¦ç›¸å…³æ€§: {corr:.4f} (å¼‚é…æ€§ - é«˜åº¦èŠ‚ç‚¹å€¾å‘äºè¿æ¥ä½åº¦èŠ‚ç‚¹)")
            else:
                report.append(f"åº¦ç›¸å…³æ€§: {corr:.4f} (ä¸­æ€§)")
        report.append("")
        
        # ç½‘ç»œç±»å‹åˆ¤æ–­
        report.append("ã€ç½‘ç»œç±»å‹åˆ¤æ–­ã€‘")
        avg_degree = self.stats['avg_degree']
        if self.stats.get('global_clustering_user', 0) > 0.1:
            clustering = self.stats.get('global_clustering_user', 0)
            avg_path = self.stats.get('avg_path_length', float('inf'))
            if clustering > 0.1 and avg_path < 10:
                report.append("âœ“ å…·æœ‰å°ä¸–ç•Œç½‘ç»œç‰¹å¾ï¼ˆé«˜èšç±»ç³»æ•° + çŸ­å¹³å‡è·¯å¾„é•¿åº¦ï¼‰")
            else:
                report.append("âœ— ä¸å…·æœ‰æ˜æ˜¾çš„å°ä¸–ç•Œç½‘ç»œç‰¹å¾")
        
        if self.stats.get('is_power_law', False):
            report.append("âœ“ å…·æœ‰æ— æ ‡åº¦ç½‘ç»œç‰¹å¾ï¼ˆåº¦åˆ†å¸ƒéµå¾ªå¹‚å¾‹åˆ†å¸ƒï¼‰")
        else:
            report.append("âœ— ä¸å…·æœ‰æ˜æ˜¾çš„æ— æ ‡åº¦ç½‘ç»œç‰¹å¾")
        report.append("")
        
        # å…³é”®èŠ‚ç‚¹
        report.append("ã€å…³é”®èŠ‚ç‚¹ã€‘")
        report.append("Top-10 é«˜å½±å“åŠ›ç”¨æˆ·:")
        for i, (node, degree) in enumerate(self.stats.get('top_users', [])[:10], 1):
            user_id = node.replace('U_', '')
            report.append(f"  {i}. ç”¨æˆ· {user_id}: åº¦ = {degree}")
        
        report.append("")
        report.append("Top-10 é«˜å½±å“åŠ›ç”µå½±:")
        for i, (node, degree) in enumerate(self.stats.get('top_movies', [])[:10], 1):
            movie_id = node.replace('M_', '')
            movie_info = self.movies[self.movies['movieId'] == int(movie_id)]
            if not movie_info.empty:
                title = movie_info.iloc[0]['title']
                report.append(f"  {i}. {title} (ID: {movie_id}): åº¦ = {degree}")
            else:
                report.append(f"  {i}. ç”µå½± {movie_id}: åº¦ = {degree}")
        
        report.append("")
        report.append("=" * 60)
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = "\n".join(report)
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        print("\n" + report_text)
    
    def run_full_analysis(self, output_dir='output', skip_clustering=False, fast_mode=False, use_gpu=None, sample_size=2000, no_sampling=False):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            skip_clustering: æ˜¯å¦è·³è¿‡èšç±»ç³»æ•°è®¡ç®—ï¼ˆå¦‚æœå¤ªæ…¢ï¼‰
            fast_mode: å¿«é€Ÿæ¨¡å¼ï¼ˆè·³è¿‡èšç±»ç³»æ•°å’Œè·¯å¾„é•¿åº¦è®¡ç®—ï¼‰
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
            sample_size: é‡‡æ ·èŠ‚ç‚¹æ•°é‡ï¼ˆç”¨äºèšç±»ç³»æ•°è®¡ç®—ï¼Œé»˜è®¤2000ï¼‰
            no_sampling: æ˜¯å¦ç¦ç”¨é‡‡æ ·ï¼Œå¼ºåˆ¶ä½¿ç”¨å…¨éƒ¨èŠ‚ç‚¹
        """
        print("=" * 60)
        print("å¼€å§‹ç½‘ç»œæ‹“æ‰‘ç‰¹æ€§åˆ†æ")
        if fast_mode:
            print("ã€å¿«é€Ÿæ¨¡å¼ï¼šå°†è·³è¿‡èšç±»ç³»æ•°å’Œè·¯å¾„é•¿åº¦è®¡ç®—ã€‘")
        elif skip_clustering:
            print("ã€å°†è·³è¿‡èšç±»ç³»æ•°è®¡ç®—ã€‘")
        if use_gpu is not None:
            print(f"ã€GPUæ¨¡å¼: {'å¯ç”¨' if use_gpu else 'ç¦ç”¨'}ã€‘")
        if no_sampling:
            print("ã€ç¦ç”¨é‡‡æ ·æ¨¡å¼ï¼šå°†ä½¿ç”¨å…¨éƒ¨èŠ‚ç‚¹è®¡ç®—ã€‘")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. æ„å»ºå›¾
        self.build_bipartite_graph()
        
        # 3. è®¡ç®—å„ç§ç»Ÿè®¡ç‰¹æ€§
        self.calculate_basic_stats()
        
        if not fast_mode and not skip_clustering:
            # ä½¿ç”¨é‡‡æ ·æ–¹æ³•è®¡ç®—èšç±»ç³»æ•°ï¼ˆæ›´å¿«ï¼‰ï¼Œæ”¯æŒGPUåŠ é€Ÿ
            # å¦‚æœè®¾ç½®äº†--no-samplingï¼Œåˆ™ç¦ç”¨é‡‡æ ·
            use_sampling = not no_sampling
            self.calculate_clustering_coefficient(use_sampling=use_sampling, sample_size=sample_size, use_gpu=use_gpu)
        else:
            print("\n=== è·³è¿‡èšç±»ç³»æ•°è®¡ç®— ===")
            self.stats['global_clustering_user'] = None
            self.stats['global_clustering_movie'] = None
        
        if not fast_mode:
            self.calculate_path_length()
        else:
            print("\n=== è·³è¿‡è·¯å¾„é•¿åº¦è®¡ç®— ===")
            self.stats['avg_path_length'] = None
            self.stats['diameter'] = None
        
        self.analyze_degree_distribution()
        self.calculate_degree_correlation()
        self.identify_key_nodes()
        
        # 4. å¯è§†åŒ–
        self.visualize_degree_distribution(save_path=str(output_path / 'network_degree_distribution.png'))
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self.generate_report(save_path=str(output_path / 'network_analysis_report.txt'))
        
        print("\n" + "=" * 60)
        print("åˆ†æå®Œæˆï¼")
        print("=" * 60)
        
        return self.stats


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ç½‘ç»œæ‹“æ‰‘ç‰¹æ€§åˆ†æ')
    parser.add_argument('--fast', action='store_true', 
                       help='å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡èšç±»ç³»æ•°å’Œè·¯å¾„é•¿åº¦è®¡ç®—ï¼ˆé€‚åˆå¤§å‹ç½‘ç»œï¼‰')
    parser.add_argument('--skip-clustering', action='store_true',
                       help='è·³è¿‡èšç±»ç³»æ•°è®¡ç®—ï¼ˆåªè·³è¿‡èšç±»ç³»æ•°ï¼Œä»è®¡ç®—è·¯å¾„é•¿åº¦ï¼‰')
    parser.add_argument('--output-dir', type=str, default='output',
                       help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputï¼‰')
    parser.add_argument('--use-gpu', action='store_true',
                       help='ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—ï¼ˆéœ€è¦å®‰è£…PyTorchï¼‰')
    parser.add_argument('--no-gpu', action='store_true',
                       help='å¼ºåˆ¶ä½¿ç”¨CPUè®¡ç®—ï¼ˆå³ä½¿GPUå¯ç”¨ï¼‰')
    parser.add_argument('--sample-size', type=int, default=2000,
                       help='é‡‡æ ·èŠ‚ç‚¹æ•°é‡ï¼ˆç”¨äºèšç±»ç³»æ•°è®¡ç®—ï¼Œé»˜è®¤2000ã€‚è¶Šå¤§è¶Šå‡†ç¡®ä½†è¶Šæ…¢ï¼‰')
    parser.add_argument('--no-sampling', action='store_true',
                       help='ç¦ç”¨é‡‡æ ·ï¼Œå¼ºåˆ¶ä½¿ç”¨å…¨éƒ¨èŠ‚ç‚¹è®¡ç®—ï¼ˆé€‚åˆèŠ‚ç‚¹æ•°<10000çš„ç½‘ç»œï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨GPU
    use_gpu = None
    if args.use_gpu:
        use_gpu = True
    elif args.no_gpu:
        use_gpu = False
    # å¦åˆ™è‡ªåŠ¨æ£€æµ‹
    
    # æ˜¾ç¤ºGPUçŠ¶æ€
    if GPU_AVAILABLE and TORCH_AVAILABLE:
        print("âœ“ PyTorch GPUå·²å®‰è£…ä¸”å¯ç”¨")
        try:
            import torch
            if torch.cuda.is_available():
                print(f"  GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        except:
            pass
    elif TORCH_AVAILABLE:
        print("âš  PyTorchå·²å®‰è£…ï¼Œä½†æœªæ£€æµ‹åˆ°GPUè®¾å¤‡")
        print("  å°†ä½¿ç”¨CPUè®¡ç®—")
    else:
        print("âš  PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUè®¡ç®—")
        print("  å¦‚éœ€GPUåŠ é€Ÿï¼Œè¯·å®‰è£…: pip install torch")
        print("  (PyTorchä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨GPU)")
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œå®Œæ•´åˆ†æ
    analyzer = NetworkAnalyzer()
    stats = analyzer.run_full_analysis(
        output_dir=args.output_dir,
        skip_clustering=args.skip_clustering,
        fast_mode=args.fast,
        use_gpu=use_gpu,
        sample_size=args.sample_size,
        no_sampling=args.no_sampling
    )
    
    print("\nåˆ†æç»“æœæ‘˜è¦:")
    print(f"- èŠ‚ç‚¹æ•°: {stats['num_nodes']}")
    print(f"- è¾¹æ•°: {stats['num_edges']}")
    print(f"- å¹³å‡åº¦: {stats['avg_degree']:.2f}")
    if stats.get('power_law_exponent'):
        print(f"- å¹‚å¾‹æŒ‡æ•°: {stats['power_law_exponent']:.2f}")
    if stats.get('global_clustering_user') is not None:
        print(f"- ç”¨æˆ·ç½‘ç»œèšç±»ç³»æ•°: {stats['global_clustering_user']:.4f}")
    if stats.get('avg_path_length') is not None:
        print(f"- å¹³å‡è·¯å¾„é•¿åº¦: {stats['avg_path_length']:.2f}")

